"""
Senses: ê°ê° ê¸°ê´€ (Peripheral Sensors)
======================================

ğŸ‘ï¸ ëˆˆ (Eyes): ì¹´ë©”ë¼, ì´ë¯¸ì§€
ğŸ‘‚ ê·€ (Ears): ë§ˆì´í¬, ìŒì„±
âŒ¨ï¸ í…ìŠ¤íŠ¸: í‚¤ë³´ë“œ ì…ë ¥

ì—­í• :
    ì™¸ë¶€ ì„¸ê³„ì˜ Raw Input â†’ ì‹œìƒ(Thalamus)ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” SensoryInputìœ¼ë¡œ ë³€í™˜

íë¦„:
    [Microphone] â†’ EarInput.listen() â†’ SensoryInput(AUDITORY)
    [Camera] â†’ EyeInput.see() â†’ SensoryInput(VISUAL)
    [Keyboard] â†’ TextInput.read() â†’ SensoryInput(SEMANTIC)

Author: GNJz (Qquarts)
Version: 1.0
"""

import time
from enum import Enum
from typing import Optional, Dict, Any, List
from dataclasses import dataclass


class SensorType(Enum):
    """ê°ê° ìœ í˜•"""
    VISUAL = "visual"       # ì‹œê° (ì¹´ë©”ë¼)
    AUDITORY = "auditory"   # ì²­ê° (ë§ˆì´í¬)
    TACTILE = "tactile"     # ì´‰ê° (ì„¼ì„œ)
    TEXT = "text"           # í…ìŠ¤íŠ¸ (í‚¤ë³´ë“œ)
    INTERNAL = "internal"   # ë‚´ë¶€ ê°ê° (ë°°í„°ë¦¬ ë“±)


@dataclass
class RawInput:
    """Raw ì„¼ì„œ ì…ë ¥"""
    sensor_type: SensorType
    data: Any
    timestamp: float
    metadata: Dict = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class EyeInput:
    """
    ğŸ‘ï¸ ëˆˆ (ì‹œê° ì…ë ¥)
    
    ì¹´ë©”ë¼ ë˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ì—ì„œ ì‹œê° ì •ë³´ ìˆ˜ì§‘
    """
    
    def __init__(self):
        self.camera = None
        self.last_frame = None
        self.is_active = False
        
        # í†µê³„
        self.stats = {
            'frames_captured': 0,
            'objects_detected': 0,
        }
    
    def activate(self) -> bool:
        """ì¹´ë©”ë¼ í™œì„±í™”"""
        try:
            # OpenCV ì‚¬ìš© (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´)
            import cv2
            self.camera = cv2.VideoCapture(0)
            self.is_active = self.camera.isOpened()
            return self.is_active
        except ImportError:
            print("âš ï¸ OpenCVê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹œê° ì…ë ¥ ë¹„í™œì„±í™”.")
            self.is_active = False
            return False
    
    def deactivate(self):
        """ì¹´ë©”ë¼ ë¹„í™œì„±í™”"""
        if self.camera:
            self.camera.release()
            self.camera = None
        self.is_active = False
    
    def see(self) -> Optional[RawInput]:
        """
        í•œ í”„ë ˆì„ ìº¡ì²˜
        
        Returns:
            RawInput(VISUAL) ë˜ëŠ” None
        """
        if not self.is_active or self.camera is None:
            return None
        
        try:
            import cv2
            ret, frame = self.camera.read()
            
            if ret:
                self.last_frame = frame
                self.stats['frames_captured'] += 1
                
                return RawInput(
                    sensor_type=SensorType.VISUAL,
                    data=frame,
                    timestamp=time.time(),
                    metadata={
                        'width': frame.shape[1],
                        'height': frame.shape[0],
                        'channels': frame.shape[2] if len(frame.shape) > 2 else 1,
                    }
                )
        except Exception as e:
            print(f"âŒ ì‹œê° ì…ë ¥ ì˜¤ë¥˜: {e}")
        
        return None
    
    def see_image(self, image_path: str) -> Optional[RawInput]:
        """ì´ë¯¸ì§€ íŒŒì¼ì—ì„œ ì½ê¸°"""
        try:
            import cv2
            frame = cv2.imread(image_path)
            
            if frame is not None:
                return RawInput(
                    sensor_type=SensorType.VISUAL,
                    data=frame,
                    timestamp=time.time(),
                    metadata={
                        'source': 'file',
                        'path': image_path,
                    }
                )
        except ImportError:
            # OpenCV ì—†ìœ¼ë©´ PIL ì‹œë„
            try:
                from PIL import Image
                import numpy as np
                img = Image.open(image_path)
                frame = np.array(img)
                
                return RawInput(
                    sensor_type=SensorType.VISUAL,
                    data=frame,
                    timestamp=time.time(),
                    metadata={'source': 'file', 'path': image_path}
                )
            except:
                pass
        
        return None


class EarInput:
    """
    ğŸ‘‚ ê·€ (ì²­ê° ì…ë ¥)
    
    ë§ˆì´í¬ì—ì„œ ìŒì„± ìˆ˜ì§‘ ë° STT(Speech-to-Text)
    """
    
    def __init__(self):
        self.microphone = None
        self.recognizer = None
        self.is_active = False
        
        # STT ì„¤ì •
        self.stt_engine = None  # 'google', 'whisper', 'local'
        
        # í†µê³„
        self.stats = {
            'recordings': 0,
            'transcriptions': 0,
        }
    
    def activate(self, stt_engine: str = 'google') -> bool:
        """ë§ˆì´í¬ í™œì„±í™”"""
        try:
            import speech_recognition as sr
            self.recognizer = sr.Recognizer()
            self.microphone = sr.Microphone()
            self.stt_engine = stt_engine
            self.is_active = True
            
            # ë…¸ì´ì¦ˆ ì¡°ì •
            with self.microphone as source:
                self.recognizer.adjust_for_ambient_noise(source, duration=1)
            
            return True
        except ImportError:
            print("âš ï¸ SpeechRecognitionì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì²­ê° ì…ë ¥ ë¹„í™œì„±í™”.")
            self.is_active = False
            return False
        except Exception as e:
            print(f"âš ï¸ ë§ˆì´í¬ í™œì„±í™” ì‹¤íŒ¨: {e}")
            self.is_active = False
            return False
    
    def deactivate(self):
        """ë§ˆì´í¬ ë¹„í™œì„±í™”"""
        self.microphone = None
        self.recognizer = None
        self.is_active = False
    
    def listen(self, timeout: float = 5.0) -> Optional[RawInput]:
        """
        ìŒì„± ë“£ê¸° (STT í¬í•¨)
        
        Returns:
            RawInput(AUDITORY) - dataì— í…ìŠ¤íŠ¸ í¬í•¨
        """
        if not self.is_active:
            return None
        
        try:
            import speech_recognition as sr
            
            with self.microphone as source:
                print("ğŸ¤ ë“£ëŠ” ì¤‘...")
                audio = self.recognizer.listen(source, timeout=timeout)
            
            self.stats['recordings'] += 1
            
            # STT
            text = self._transcribe(audio)
            
            if text:
                self.stats['transcriptions'] += 1
                return RawInput(
                    sensor_type=SensorType.AUDITORY,
                    data=text,
                    timestamp=time.time(),
                    metadata={
                        'stt_engine': self.stt_engine,
                        'audio_duration': len(audio.frame_data) / audio.sample_rate,
                    }
                )
        except Exception as e:
            print(f"âŒ ì²­ê° ì…ë ¥ ì˜¤ë¥˜: {e}")
        
        return None
    
    def _transcribe(self, audio) -> Optional[str]:
        """ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜"""
        try:
            if self.stt_engine == 'google':
                return self.recognizer.recognize_google(audio, language='ko-KR')
            elif self.stt_engine == 'whisper':
                return self.recognizer.recognize_whisper(audio, language='ko')
            else:
                return self.recognizer.recognize_google(audio, language='ko-KR')
        except Exception as e:
            print(f"âš ï¸ STT ì‹¤íŒ¨: {e}")
            return None
    
    def listen_from_file(self, audio_path: str) -> Optional[RawInput]:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ì½ê¸°"""
        if not self.recognizer:
            try:
                import speech_recognition as sr
                self.recognizer = sr.Recognizer()
            except ImportError:
                return None
        
        try:
            import speech_recognition as sr
            
            with sr.AudioFile(audio_path) as source:
                audio = self.recognizer.record(source)
            
            text = self._transcribe(audio)
            
            if text:
                return RawInput(
                    sensor_type=SensorType.AUDITORY,
                    data=text,
                    timestamp=time.time(),
                    metadata={'source': 'file', 'path': audio_path}
                )
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        return None


class TextInput:
    """
    âŒ¨ï¸ í…ìŠ¤íŠ¸ ì…ë ¥
    
    í‚¤ë³´ë“œ ë˜ëŠ” íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    ê°€ì¥ ê¸°ë³¸ì ì¸ ì…ë ¥ ë°©ì‹
    """
    
    def __init__(self):
        self.buffer: List[str] = []
        
        # í†µê³„
        self.stats = {
            'inputs_received': 0,
            'total_chars': 0,
        }
    
    def read(self, text: str) -> RawInput:
        """
        í…ìŠ¤íŠ¸ ì…ë ¥ ì²˜ë¦¬
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            RawInput(TEXT)
        """
        self.stats['inputs_received'] += 1
        self.stats['total_chars'] += len(text)
        self.buffer.append(text)
        
        return RawInput(
            sensor_type=SensorType.TEXT,
            data=text,
            timestamp=time.time(),
            metadata={
                'length': len(text),
                'words': len(text.split()),
            }
        )
    
    def read_file(self, file_path: str, encoding: str = 'utf-8') -> Optional[RawInput]:
        """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì½ê¸°"""
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                text = f.read()
            
            return self.read(text)
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            return None
    
    def get_history(self, n: int = 10) -> List[str]:
        """ìµœê·¼ ì…ë ¥ ê¸°ë¡"""
        return self.buffer[-n:]


class Senses:
    """
    ğŸ­ í†µí•© ê°ê° ì‹œìŠ¤í…œ
    
    ëª¨ë“  ê°ê° ê¸°ê´€ì„ í†µí•© ê´€ë¦¬
    """
    
    def __init__(self):
        self.eyes = EyeInput()
        self.ears = EarInput()
        self.text = TextInput()
        
        # ë‚´ë¶€ ì„¼ì„œ (ë°°í„°ë¦¬, ì˜¨ë„ ë“±)
        self.internal_state = {
            'battery': 1.0,
            'temperature': 0.5,  # ì •ìƒ ë²”ìœ„
        }
        
        # í†µê³„
        self.stats = {
            'total_inputs': 0,
        }
    
    def activate_all(self) -> Dict[str, bool]:
        """ëª¨ë“  ì„¼ì„œ í™œì„±í™”"""
        return {
            'eyes': self.eyes.activate(),
            'ears': self.ears.activate(),
            'text': True,  # í…ìŠ¤íŠ¸ëŠ” í•­ìƒ í™œì„±
        }
    
    def deactivate_all(self):
        """ëª¨ë“  ì„¼ì„œ ë¹„í™œì„±í™”"""
        self.eyes.deactivate()
        self.ears.deactivate()
    
    def sense(self, modality: SensorType, data: Any = None) -> Optional[RawInput]:
        """
        ê°ê° ìˆ˜ì§‘ (í†µí•© ì¸í„°í˜ì´ìŠ¤)
        
        Args:
            modality: ê°ê° ìœ í˜•
            data: í…ìŠ¤íŠ¸ì˜ ê²½ìš° ì§ì ‘ ë°ì´í„° ì „ë‹¬
            
        Returns:
            RawInput ë˜ëŠ” None
        """
        self.stats['total_inputs'] += 1
        
        if modality == SensorType.VISUAL:
            return self.eyes.see()
        elif modality == SensorType.AUDITORY:
            return self.ears.listen()
        elif modality == SensorType.TEXT:
            if data:
                return self.text.read(data)
        elif modality == SensorType.INTERNAL:
            return RawInput(
                sensor_type=SensorType.INTERNAL,
                data=self.internal_state.copy(),
                timestamp=time.time()
            )
        
        return None
    
    def to_sensory_input(self, raw: RawInput):
        """
        RawInput â†’ SensoryInput ë³€í™˜ (ì‹œìƒìš©)
        
        brain/_1_thalamus.pyì˜ SensoryInput í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        """
        from ..brain import SensoryInput, ModalityType
        
        # SensorType â†’ ModalityType ë§¤í•‘
        modality_map = {
            SensorType.VISUAL: ModalityType.VISUAL,
            SensorType.AUDITORY: ModalityType.AUDITORY,
            SensorType.TEXT: ModalityType.SEMANTIC,
            SensorType.INTERNAL: ModalityType.SEMANTIC,
        }
        
        modality = modality_map.get(raw.sensor_type, ModalityType.SEMANTIC)
        
        # ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì‹œìƒì´ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡)
        if isinstance(raw.data, str):
            content = raw.data
        elif raw.sensor_type == SensorType.VISUAL:
            content = f"[ì´ë¯¸ì§€: {raw.metadata.get('width', '?')}x{raw.metadata.get('height', '?')}]"
        elif raw.sensor_type == SensorType.INTERNAL:
            content = f"[ë‚´ë¶€ìƒíƒœ: ë°°í„°ë¦¬={raw.data.get('battery', '?'):.0%}]"
        else:
            content = str(raw.data)
        
        return SensoryInput(
            modality=modality,
            content=content,
            intensity=0.7,  # ê¸°ë³¸ ê°•ë„
            timestamp=raw.timestamp
        )
    
    def get_stats(self) -> Dict:
        """í†µê³„"""
        return {
            'total_inputs': self.stats['total_inputs'],
            'eyes': self.eyes.stats,
            'ears': self.ears.stats,
            'text': self.text.stats,
        }

