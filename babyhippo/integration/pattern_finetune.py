"""
=============================================================================
Pattern Fine-Tuning: í•™ìŠµëœ íŒ¨í„´ ê¸°ë°˜ LLM íŒŒì¸íŠœë‹
=============================================================================

ğŸŒŠ ì² í•™:
    "í•˜ë“œì½”ë”©ì€ ì£½ìŒì´ë‹¤"
    "íŒ¨í„´ì€ ë°œê²¬ë˜ëŠ” ê²ƒì´ì§€ ì§€ì •ë˜ëŠ” ê²ƒì´ ì•„ë‹˜"
    "í•™ìŠµëœ ê²½í—˜ì´ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ê²©ì´ ëœë‹¤"

ğŸ“ í•µì‹¬ ì›ë¦¬:

    1. íŒ¨í„´ ìˆ˜ì§‘ (Pattern Collection)
       - HippoMemoryì˜ ê³µê³ í™”ëœ ê¸°ì–µ
       - ResponseMemoryì˜ í•™ìŠµëœ ì‘ë‹µ
       - PatternMemoryì˜ ë°œê²¬ëœ íŒ¨í„´
       
    2. ë°ì´í„° ë³€í™˜ (Data Transformation)
       - íŒ¨í„´ â†’ ìì—°ì–´ ë¬¸ì¥
       - ì§ˆë¬¸-ì‘ë‹µ ìŒ ìƒì„±
       - ë§¥ë½ ì •ë³´ í¬í•¨
       
    3. íŒŒì¸íŠœë‹ ë°ì´í„° ìƒì„± (Training Data Generation)
       - nanoGPT í˜•ì‹ (char-level)
       - ì¤‘ìš”ë„ ê¸°ë°˜ ë°˜ë³µ (ì¤‘ìš”í•œ íŒ¨í„´ ë” ë§ì´)
       - ë…¸ì´ì¦ˆ ì¶”ê°€ (ë³€í˜•ìœ¼ë¡œ ì¼ë°˜í™”)

ìƒë¬¼í•™ì  ê·¼ê±°:
    - Systems Consolidation Theory
    - í•´ë§ˆ â†’ ëŒ€ë‡Œí”¼ì§ˆ ì „ì´
    - ë°˜ë³µ ë…¸ì¶œë¡œ ì¥ê¸° ê¸°ì–µ í˜•ì„±

Author: GNJz (Qquarts)
Version: 1.0.0
=============================================================================
"""

import os
import sys
import json
import time
import random
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field, asdict
from datetime import datetime

# ê²½ë¡œ ì„¤ì •
BABYHIPPO_PATH = Path(__file__).parent.parent
NANOGPT_PATH = BABYHIPPO_PATH.parent / "nanoGPT"


@dataclass
class TrainingSample:
    """
    í•™ìŠµ ìƒ˜í”Œ
    
    Attributes:
        text: í•™ìŠµ í…ìŠ¤íŠ¸
        source: ì¶œì²˜ (hippo/response/pattern)
        importance: ì¤‘ìš”ë„ (0~1)
        pattern_id: ê´€ë ¨ íŒ¨í„´ ID
        metadata: ì¶”ê°€ ì •ë³´
    """
    text: str
    source: str = "unknown"
    importance: float = 0.5
    pattern_id: Optional[str] = None
    metadata: Dict = field(default_factory=dict)


class PatternCollector:
    """
    íŒ¨í„´ ìˆ˜ì§‘ê¸°
    
    ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ í•™ìŠµëœ íŒ¨í„´ì„ ìˆ˜ì§‘:
    - HippoMemory (ê³µê³ í™”ëœ ê¸°ì–µ)
    - ResponseMemory (í•™ìŠµëœ ì‘ë‹µ)
    - PatternMemory (ë°œê²¬ëœ íŒ¨í„´)
    """
    
    def __init__(self, 
                 hippo_memory=None,
                 response_memory=None,
                 pattern_memory=None):
        self.hippo = hippo_memory
        self.response_memory = response_memory
        self.pattern_memory = pattern_memory
        
    def collect_from_hippo(self, consolidation_threshold: float = 0.5) -> List[TrainingSample]:
        """
        HippoMemoryì—ì„œ ê³µê³ í™”ëœ ê¸°ì–µ ìˆ˜ì§‘
        
        Args:
            consolidation_threshold: ê³µê³ í™” ì„ê³„ê°’
            
        Returns:
            í•™ìŠµ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
        """
        samples = []
        
        if not self.hippo or not hasattr(self.hippo, 'words'):
            return samples
        
        for word_id, word_info in self.hippo.words.items():
            # ê³µê³ í™” ìˆ˜ì¤€ í™•ì¸
            synapses = word_info.get('synapses_dg_ca3', [])
            if not synapses:
                continue
            
            avg_consolidation = np.mean([
                getattr(syn, 'consolidation_level', 0) for syn in synapses
            ])
            
            if avg_consolidation >= consolidation_threshold:
                # ì¤‘ìš”ë„ ê³„ì‚°
                try:
                    importance = self.hippo.memory_ranker.get_score(word_id, default=0.5)
                except:
                    importance = 0.5
                
                context = self.hippo.contexts.get(word_id, '')
                frequency = self.hippo.word_frequencies.get(word_id, 1)
                
                # í…ìŠ¤íŠ¸ ìƒì„±
                text = word_id
                if context:
                    text = f"[{context}] {text}"
                
                samples.append(TrainingSample(
                    text=text,
                    source="hippo",
                    importance=importance,
                    pattern_id=word_id,
                    metadata={
                        'consolidation': avg_consolidation,
                        'frequency': frequency,
                        'context': context,
                    }
                ))
        
        return samples
    
    def collect_from_response_memory(self, usage_threshold: int = 3) -> List[TrainingSample]:
        """
        ResponseMemoryì—ì„œ ìì£¼ ì‚¬ìš©ëœ ì‘ë‹µ íŒ¨í„´ ìˆ˜ì§‘
        
        Args:
            usage_threshold: ìµœì†Œ ì‚¬ìš© íšŸìˆ˜
            
        Returns:
            í•™ìŠµ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
        """
        samples = []
        
        if not self.response_memory:
            return samples
        
        for category, responses in self.response_memory.responses.items():
            for lr in responses:
                if lr.usage_count >= usage_threshold:
                    # íŠ¸ë¦¬ê±° + ì‘ë‹µ ìŒìœ¼ë¡œ ë³€í™˜
                    triggers = lr.triggers or [category]
                    
                    for trigger in triggers:
                        # Q&A í˜•ì‹
                        text = f"Q: {trigger}\nA: {lr.response}"
                        
                        samples.append(TrainingSample(
                            text=text,
                            source="response",
                            importance=min(1.0, lr.success_score * 0.5),
                            pattern_id=f"resp_{category}_{id(lr)}",
                            metadata={
                                'category': category,
                                'usage_count': lr.usage_count,
                                'success_score': lr.success_score,
                            }
                        ))
        
        return samples
    
    def collect_from_pattern_memory(self) -> List[TrainingSample]:
        """
        PatternMemoryì—ì„œ í•™ìŠµëœ íŒ¨í„´ ìˆ˜ì§‘
        
        Returns:
            í•™ìŠµ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
        """
        samples = []
        
        if not self.pattern_memory:
            return samples
        
        for pattern_id, pattern in self.pattern_memory.patterns.items():
            # íŒ¨í„´ì˜ ì—°ê´€ ë ˆì´ë¸”ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
            labels = pattern.associated_labels or [pattern_id]
            text = " ".join(labels)
            
            samples.append(TrainingSample(
                text=text,
                source="pattern",
                importance=min(1.0, pattern.strength / 5.0),
                pattern_id=pattern_id,
                metadata={
                    'activation_count': pattern.activation_count,
                    'strength': pattern.strength,
                }
            ))
        
        return samples
    
    def collect_all(self, 
                    hippo_threshold: float = 0.5,
                    response_threshold: int = 3) -> List[TrainingSample]:
        """
        ëª¨ë“  ì†ŒìŠ¤ì—ì„œ íŒ¨í„´ ìˆ˜ì§‘
        
        Returns:
            í†µí•©ëœ í•™ìŠµ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
        """
        all_samples = []
        
        # HippoMemory
        hippo_samples = self.collect_from_hippo(hippo_threshold)
        all_samples.extend(hippo_samples)
        print(f"   ğŸ“¦ HippoMemory: {len(hippo_samples)}ê°œ")
        
        # ResponseMemory
        response_samples = self.collect_from_response_memory(response_threshold)
        all_samples.extend(response_samples)
        print(f"   ğŸ“¦ ResponseMemory: {len(response_samples)}ê°œ")
        
        # PatternMemory
        pattern_samples = self.collect_from_pattern_memory()
        all_samples.extend(pattern_samples)
        print(f"   ğŸ“¦ PatternMemory: {len(pattern_samples)}ê°œ")
        
        return all_samples


class TrainingDataGenerator:
    """
    í•™ìŠµ ë°ì´í„° ìƒì„±ê¸°
    
    ìˆ˜ì§‘ëœ íŒ¨í„´ì„ nanoGPT í•™ìŠµ ë°ì´í„°ë¡œ ë³€í™˜
    """
    
    def __init__(self, noise_level: float = 0.1):
        self.noise_level = noise_level
        
    def augment_text(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ ì¦ê°• (ë³€í˜•ìœ¼ë¡œ ì¼ë°˜í™”)
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ë³€í˜•ëœ í…ìŠ¤íŠ¸ë“¤
        """
        variations = [text]  # ì›ë³¸ í¬í•¨
        
        # 1. ê³µë°± ë³€í˜•
        if ' ' in text:
            variations.append(text.replace(' ', '  '))  # ë”ë¸” ìŠ¤í˜ì´ìŠ¤
        
        # 2. ëŒ€ì†Œë¬¸ì ë³€í˜•
        if any(c.isalpha() for c in text):
            variations.append(text.lower())
            variations.append(text.upper())
        
        # 3. ìˆœì„œ ì…”í”Œ (ë‹¨ì–´ ë‹¨ìœ„)
        words = text.split()
        if len(words) > 2:
            shuffled = words.copy()
            random.shuffle(shuffled)
            variations.append(' '.join(shuffled))
        
        return variations
    
    def generate_training_text(self, 
                               samples: List[TrainingSample],
                               repeat_by_importance: bool = True,
                               augment: bool = True) -> str:
        """
        í•™ìŠµ ë°ì´í„° í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            samples: í•™ìŠµ ìƒ˜í”Œë“¤
            repeat_by_importance: ì¤‘ìš”ë„ ê¸°ë°˜ ë°˜ë³µ
            augment: ë°ì´í„° ì¦ê°• ì—¬ë¶€
            
        Returns:
            í•™ìŠµìš© í…ìŠ¤íŠ¸
        """
        lines = []
        
        for sample in samples:
            # ì¤‘ìš”ë„ ê¸°ë°˜ ë°˜ë³µ íšŸìˆ˜ ê²°ì •
            if repeat_by_importance:
                repeat_count = max(1, int(sample.importance * 5))
            else:
                repeat_count = 1
            
            # í…ìŠ¤íŠ¸ ì¶”ê°€
            for _ in range(repeat_count):
                if augment:
                    variations = self.augment_text(sample.text)
                    lines.extend(variations)
                else:
                    lines.append(sample.text)
        
        # ì…”í”Œ (í•™ìŠµ ì•ˆì •ì„±)
        random.shuffle(lines)
        
        return '\n'.join(lines)
    
    def export_to_nanogpt(self, 
                          samples: List[TrainingSample],
                          output_dir: str = None) -> Dict[str, str]:
        """
        nanoGPT í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
        
        Args:
            samples: í•™ìŠµ ìƒ˜í”Œë“¤
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            ìƒì„±ëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        if output_dir is None:
            output_dir = NANOGPT_PATH / "data" / "hippo_patterns"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # í•™ìŠµ ë°ì´í„° ìƒì„±
        train_text = self.generate_training_text(
            samples, 
            repeat_by_importance=True,
            augment=True
        )
        
        # íŒŒì¼ ì €ì¥
        train_path = output_dir / "train.txt"
        with open(train_path, 'w', encoding='utf-8') as f:
            f.write(train_text)
        
        # ë©”íƒ€ ì •ë³´ ì €ì¥
        meta = {
            'created_at': datetime.now().isoformat(),
            'samples_count': len(samples),
            'total_lines': len(train_text.split('\n')),
            'sources': list(set(s.source for s in samples)),
        }
        
        meta_path = output_dir / "meta.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        
        return {
            'train': str(train_path),
            'meta': str(meta_path),
        }


class PatternFineTuner:
    """
    íŒ¨í„´ íŒŒì¸íŠœë„ˆ
    
    í•™ìŠµëœ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ LLM íŒŒì¸íŠœë‹
    """
    
    def __init__(self):
        self.collector = None
        self.generator = TrainingDataGenerator()
        self.samples: List[TrainingSample] = []
        
    def setup(self, 
              hippo_memory=None,
              response_memory=None,
              pattern_memory=None):
        """
        ì†ŒìŠ¤ ì„¤ì •
        
        Args:
            hippo_memory: HippoMemory ì¸ìŠ¤í„´ìŠ¤
            response_memory: ResponseMemory ì¸ìŠ¤í„´ìŠ¤
            pattern_memory: PatternMemory ì¸ìŠ¤í„´ìŠ¤
        """
        self.collector = PatternCollector(
            hippo_memory=hippo_memory,
            response_memory=response_memory,
            pattern_memory=pattern_memory
        )
    
    def collect_patterns(self) -> int:
        """
        íŒ¨í„´ ìˆ˜ì§‘
        
        Returns:
            ìˆ˜ì§‘ëœ ìƒ˜í”Œ ìˆ˜
        """
        if not self.collector:
            print("âš ï¸ setup()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”")
            return 0
        
        print("ğŸ“Š íŒ¨í„´ ìˆ˜ì§‘ ì¤‘...")
        self.samples = self.collector.collect_all()
        print(f"   ì´ {len(self.samples)}ê°œ ìƒ˜í”Œ ìˆ˜ì§‘ë¨")
        
        return len(self.samples)
    
    def prepare_training_data(self, output_dir: str = None) -> Dict[str, str]:
        """
        í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            ìƒì„±ëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        if not self.samples:
            print("âš ï¸ collect_patterns()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”")
            return {}
        
        print("ğŸ“ í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘...")
        paths = self.generator.export_to_nanogpt(self.samples, output_dir)
        
        print(f"   âœ… í•™ìŠµ ë°ì´í„° ìƒì„± ì™„ë£Œ")
        print(f"      train: {paths['train']}")
        print(f"      meta: {paths['meta']}")
        
        return paths
    
    def generate_finetune_script(self, output_path: str = None) -> str:
        """
        íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        
        Args:
            output_path: ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
            
        Returns:
            ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
        """
        if output_path is None:
            output_path = NANOGPT_PATH / "finetune_patterns.sh"
        
        script = f"""#!/bin/bash
# =============================================================================
# babyhippo Pattern Fine-Tuning Script
# í•™ìŠµëœ íŒ¨í„´ìœ¼ë¡œ nanoGPT íŒŒì¸íŠœë‹
# =============================================================================

cd {NANOGPT_PATH}

# 1. ë°ì´í„° ì¤€ë¹„
echo "ğŸ“¦ ë°ì´í„° ì¤€ë¹„..."
python3 data/hippo_patterns/prepare.py

# 2. íŒŒì¸íŠœë‹ (ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜)
echo "ğŸ”§ íŒŒì¸íŠœë‹ ì‹œì‘..."
python3 train.py \\
    --dataset=hippo_patterns \\
    --init_from=resume \\
    --out_dir=out-hippo-patterns \\
    --eval_interval=50 \\
    --max_iters=500 \\
    --learning_rate=1e-4 \\
    --batch_size=8

echo "âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!"
echo "   ëª¨ë¸ ê²½ë¡œ: out-hippo-patterns/ckpt.pt"
"""
        
        with open(output_path, 'w') as f:
            f.write(script)
        
        # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
        os.chmod(output_path, 0o755)
        
        return str(output_path)
    
    def run_pipeline(self, 
                     hippo_memory=None,
                     response_memory=None,
                     pattern_memory=None,
                     output_dir: str = None) -> Dict[str, Any]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            hippo_memory: HippoMemory ì¸ìŠ¤í„´ìŠ¤
            response_memory: ResponseMemory ì¸ìŠ¤í„´ìŠ¤
            pattern_memory: PatternMemory ì¸ìŠ¤í„´ìŠ¤
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            íŒŒì´í”„ë¼ì¸ ê²°ê³¼
        """
        print("=" * 60)
        print("ğŸŒŠ Pattern Fine-Tuning Pipeline")
        print("=" * 60)
        
        # 1. ì„¤ì •
        print("\n1ï¸âƒ£ ì„¤ì •...")
        self.setup(hippo_memory, response_memory, pattern_memory)
        
        # 2. íŒ¨í„´ ìˆ˜ì§‘
        print("\n2ï¸âƒ£ íŒ¨í„´ ìˆ˜ì§‘...")
        sample_count = self.collect_patterns()
        
        if sample_count == 0:
            print("   âš ï¸ ìˆ˜ì§‘ëœ íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤")
            return {'status': 'no_patterns', 'samples': 0}
        
        # 3. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        print("\n3ï¸âƒ£ í•™ìŠµ ë°ì´í„° ì¤€ë¹„...")
        paths = self.prepare_training_data(output_dir)
        
        # 4. íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        print("\n4ï¸âƒ£ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±...")
        script_path = self.generate_finetune_script()
        print(f"   ìŠ¤í¬ë¦½íŠ¸: {script_path}")
        
        print("\n" + "=" * 60)
        print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"   ìƒ˜í”Œ ìˆ˜: {sample_count}ê°œ")
        print(f"   í•™ìŠµ ë°ì´í„°: {paths.get('train', 'N/A')}")
        print(f"   íŒŒì¸íŠœë‹ ì‹¤í–‰: bash {script_path}")
        print("=" * 60)
        
        return {
            'status': 'completed',
            'samples': sample_count,
            'paths': paths,
            'script': script_path,
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„"""
        if not self.samples:
            return {'samples': 0}
        
        sources = {}
        for s in self.samples:
            sources[s.source] = sources.get(s.source, 0) + 1
        
        importances = [s.importance for s in self.samples]
        
        return {
            'samples': len(self.samples),
            'sources': sources,
            'avg_importance': np.mean(importances),
            'max_importance': max(importances),
            'min_importance': min(importances),
        }


# =============================================================================
# í…ŒìŠ¤íŠ¸
# =============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸŒŠ Pattern Fine-Tuning Test")
    print("=" * 60)
    
    # 1. ê°€ìƒ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£ ê°€ìƒ ìƒ˜í”Œ ìƒì„±...")
    samples = [
        TrainingSample("ì•ˆë…•í•˜ì„¸ìš”", source="hippo", importance=0.9),
        TrainingSample("Q: ì´ë¦„\nA: babyhippoì…ë‹ˆë‹¤", source="response", importance=0.8),
        TrainingSample("ê³ ì–‘ì´ ê°•ì•„ì§€ ë™ë¬¼", source="pattern", importance=0.7),
    ]
    
    # 2. í•™ìŠµ ë°ì´í„° ìƒì„±
    print("\n2ï¸âƒ£ í•™ìŠµ ë°ì´í„° ìƒì„±...")
    generator = TrainingDataGenerator()
    text = generator.generate_training_text(samples, augment=True)
    print(f"   ìƒì„±ëœ ë¼ì¸ ìˆ˜: {len(text.split(chr(10)))}")
    
    # 3. íŒŒì¸íŠœë„ˆ í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ íŒŒì¸íŠœë„ˆ í…ŒìŠ¤íŠ¸...")
    finetuner = PatternFineTuner()
    finetuner.samples = samples  # ì§ì ‘ ì£¼ì…
    
    stats = finetuner.get_stats()
    print(f"   í†µê³„: {stats}")
    
    print("\n" + "=" * 60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)

