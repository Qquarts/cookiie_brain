"""
Basal Ganglia: ê¸°ì €í•µ - í–‰ë™ ì„ íƒ(Action Selection) & ìŠµê´€ í˜•ì„±(Habit)
====================================================================

ğŸ§  ìƒë¬¼í•™ì  ëª¨ë¸:
    ê¸°ì €í•µ = ë‡Œì˜ "í–‰ë™ ê²Œì´íŠ¸í‚¤í¼"
    
    1. í–‰ë™ ì„ íƒ (Action Selection)
       - ì—¬ëŸ¬ í–‰ë™ ì˜µì…˜ ì¤‘ í•˜ë‚˜ë§Œ ì‹¤í–‰ (Go/NoGo)
       - ë‚˜ë¨¸ì§€ëŠ” ì–µì œ
       
    2. ìŠµê´€ í˜•ì„± (Habit Formation)
       - ë°˜ë³µëœ í–‰ë™ â†’ ìë™í™”
       - ì „ë‘ì—½ ìš°íšŒ â†’ ë¹ ë¥¸ ì‹¤í–‰
       
    3. ë³´ìƒ í•™ìŠµ (Reward Learning)
       - ë„íŒŒë¯¼ ì‹ í˜¸ ê¸°ë°˜
       - Q-Learningê³¼ ìœ ì‚¬

ğŸ“ í•µì‹¬ ìˆ˜ì‹:
    Q-value ì—…ë°ì´íŠ¸: Q(s,a) â† Q(s,a) + Î±[R + Î³Â·max(Q(s',a')) - Q(s,a)]
    í–‰ë™ ì„ íƒ: P(a) = softmax(Q(s,a) / Ï„)
    ìŠµê´€ ê°•ë„: H = H + Î²Â·(success - H)

ğŸ“š ì°¸ê³  ë…¼ë¬¸:
    - Schultz (1997): Dopamine reward prediction
    - Graybiel (2008): Habits, rituals, and the evaluative brain
    - Frank (2005): Go/NoGo model of basal ganglia

Author: GNJz (Qquarts)
Version: 1.1
"""

import math
import time
import random
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field
from collections import defaultdict
from enum import Enum


# ============================================
# ë°ì´í„° í´ë˜ìŠ¤
# ============================================

class ActionType(Enum):
    """í–‰ë™ íƒ€ì…"""
    GO = "GO"           # ì‹¤í–‰
    NOGO = "NOGO"       # ì–µì œ
    EXPLORE = "EXPLORE" # íƒìƒ‰ (ìƒˆë¡œìš´ ì‹œë„)


@dataclass
class Action:
    """í–‰ë™"""
    name: str
    context: str = ""           # ìƒí™©/ë§¥ë½
    q_value: float = 0.0        # Q-ê°’ (ì˜ˆìƒ ë³´ìƒ)
    execution_count: int = 0    # ì‹¤í–‰ íšŸìˆ˜
    success_count: int = 0      # ì„±ê³µ íšŸìˆ˜
    habit_strength: float = 0.0 # ìŠµê´€ ê°•ë„ (0~1)
    last_executed: float = field(default_factory=time.time)
    
    @property
    def success_rate(self) -> float:
        if self.execution_count == 0:
            return 0.0
        return self.success_count / self.execution_count
    
    @property
    def is_habit(self) -> bool:
        """ìŠµê´€í™” ì—¬ë¶€ (ê°•ë„ 0.7 ì´ìƒ)"""
        return self.habit_strength >= 0.7


@dataclass
class ActionResult:
    """í–‰ë™ ê²°ê³¼"""
    action: Action
    decision: ActionType
    confidence: float           # í™•ì‹ ë„
    is_automatic: bool          # ìë™ ì‹¤í–‰ ì—¬ë¶€ (ìŠµê´€)
    reasoning: str              # ì„ íƒ ì´ìœ 


# ============================================
# ê¸°ì €í•µ í•µì‹¬ í´ë˜ìŠ¤
# ============================================

class BasalGanglia:
    """
    ê¸°ì €í•µ (Basal Ganglia)
    
    í–‰ë™ ì„ íƒ ë° ìŠµê´€ í˜•ì„± ì‹œìŠ¤í…œ
    
    êµ¬ì¡°:
        Striatum (ì„ ì¡°ì²´) - ì…ë ¥, ìƒí™©-í–‰ë™ ë§¤í•‘
        GPi/SNr (ë‹´ì°½êµ¬) - ì¶œë ¥, Go/NoGo ê²°ì •
        STN (ì‹œìƒí•˜í•µ) - ì–µì œ ì¡°ì ˆ
        
    í•™ìŠµ:
        ë„íŒŒë¯¼ ì‹ í˜¸ ê¸°ë°˜ ê°•í™”í•™ìŠµ (TD-Learning)
    """
    
    def __init__(self, bias: Optional[Dict] = None, use_hash: bool = False):
        """
        ê¸°ì €í•µ ì´ˆê¸°í™”
        
        Args:
            bias: í–‰ë™ ì„±í–¥ (DNA ì„¤ì •ê°’ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •)
                  - impulsivity (0~1): ë†’ìœ¼ë©´ íƒìƒ‰â†‘, ìŠµê´€ í˜•ì„±â†‘
                  - patience (0~1): ë†’ìœ¼ë©´ ë¯¸ë˜ ë³´ìƒ ì¤‘ì‹œ
            use_hash: ê¸´ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•´ì‹œë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ ìµœì í™”)
        
        Note:
            v1.1: ì™¸ë¶€ Bias ì£¼ì… ì§€ì› (Stem Code ì² í•™)
            - ê¸°ë³¸ê°’ = ì„ ì²œì  ì„±í–¥ (ì¤„ê¸°)
            - ì™¸ë¶€ ì£¼ì… = í™˜ê²½ì— ë”°ë¥¸ ë¶„í™”
        """
        # ===== Q-í…Œì´ë¸” (ìƒí™© â†’ í–‰ë™ â†’ ê°€ì¹˜) =====
        # {context: {action_name: Action}}
        self.q_table: Dict[str, Dict[str, Action]] = defaultdict(dict)
        
        # ===== í•˜ì´í¼íŒŒë¼ë¯¸í„° (Stem: ê¸°ë³¸ ì„±í–¥) =====
        self.params = {
            'alpha': 0.1,           # í•™ìŠµë¥ 
            'gamma': 0.9,           # í• ì¸ìœ¨ (ë¯¸ë˜ ë³´ìƒ)
            'tau': 0.5,             # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì˜¨ë„ (íƒìƒ‰ vs í™œìš©)
            'habit_threshold': 0.7,  # ìŠµê´€í™” ì„ê³„ê°’
            'habit_beta': 0.1,      # ìŠµê´€ ê°•í™”ìœ¨
            'decay_rate': 0.01,     # Q-ê°’ ê°ì‡ ìœ¨
            'exploration_bonus': 0.2, # íƒìƒ‰ ë³´ë„ˆìŠ¤
        }
        
        # [v1.1] DNA Bias ì£¼ì…
        if bias:
            # ì¶©ë™ì„±(impulsivity) ë†’ìœ¼ë©´ â†’ íƒìƒ‰â†‘, ìŠµê´€ í˜•ì„±â†‘
            if 'impulsivity' in bias:
                imp = max(0, min(1, bias['impulsivity']))  # 0~1ë¡œ í´ë¨í”„
                self.params['tau'] = 0.5 + (imp * 0.5)  # 0.5~1.0
                self.params['habit_threshold'] = 0.7 - (imp * 0.2)  # 0.5~0.7
            
            # ì¸ë‚´ì‹¬(patience) ë†’ìœ¼ë©´ â†’ ë¯¸ë˜ ë³´ìƒ ì¤‘ì‹œ
            if 'patience' in bias:
                pat = max(0, min(1, bias['patience']))  # 0~1ë¡œ í´ë¨í”„
                self.params['gamma'] = 0.8 + (pat * 0.15)  # 0.8~0.95
        
        # [v1.1] ì»¨í…ìŠ¤íŠ¸ í•´ì‹± ëª¨ë“œ
        self.use_hash = use_hash
        
        # ===== ë„íŒŒë¯¼ ìƒíƒœ =====
        self.dopamine_level = 0.5  # í˜„ì¬ ë„íŒŒë¯¼ (0~1)
        self.dopamine_baseline = 0.5
        
        # ===== ìµœê·¼ í–‰ë™ ê¸°ë¡ =====
        self.recent_actions: List[Tuple[str, str, float]] = []  # (context, action, reward)
        self.max_history = 100
        
        # ===== í†µê³„ =====
        self.stats = {
            'total_decisions': 0,
            'habit_executions': 0,
            'deliberate_executions': 0,
            'explorations': 0,
            'total_reward': 0.0,
        }
    
    # ============================================
    # 1. í–‰ë™ ì„ íƒ (Action Selection)
    # ============================================
    
    def select_action(self, 
                      context: str, 
                      possible_actions: List[str],
                      allow_exploration: bool = True) -> ActionResult:
        """
        í–‰ë™ ì„ íƒ (Go/NoGo/Explore)
        
        1. ìŠµê´€ ì²´í¬ â†’ ìë™ ì‹¤í–‰
        2. Q-ê°’ ê¸°ë°˜ ì„ íƒ â†’ ì˜ì‹ì  ê²°ì •
        3. íƒìƒ‰ â†’ ìƒˆë¡œìš´ ì‹œë„
        
        Args:
            context: í˜„ì¬ ìƒí™©/ë§¥ë½
            possible_actions: ê°€ëŠ¥í•œ í–‰ë™ ëª©ë¡
            allow_exploration: íƒìƒ‰ í—ˆìš© ì—¬ë¶€
            
        Returns:
            ActionResult
        """
        self.stats['total_decisions'] += 1
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ê·œí™”
        context = self._normalize_context(context)
        
        # 1. ìŠµê´€ ì²´í¬ (Fast Path)
        habit_action = self._check_habit(context, possible_actions)
        if habit_action:
            self.stats['habit_executions'] += 1
            return ActionResult(
                action=habit_action,
                decision=ActionType.GO,
                confidence=habit_action.habit_strength,
                is_automatic=True,
                reasoning=f"ìŠµê´€: '{habit_action.name}' (ê°•ë„: {habit_action.habit_strength:.2f})"
            )
        
        # 2. Q-ê°’ ê¸°ë°˜ ì„ íƒ (Slow Path)
        actions = self._get_or_create_actions(context, possible_actions)
        
        if not actions:
            # í–‰ë™ ì—†ìŒ
            return ActionResult(
                action=Action(name="none", context=context),
                decision=ActionType.NOGO,
                confidence=0.0,
                is_automatic=False,
                reasoning="ê°€ëŠ¥í•œ í–‰ë™ ì—†ìŒ"
            )
        
        # íƒìƒ‰ vs í™œìš© ê²°ì •
        if allow_exploration and self._should_explore():
            # íƒìƒ‰: ëœë¤ ë˜ëŠ” ë‚®ì€ Q-ê°’ í–‰ë™
            self.stats['explorations'] += 1
            action = self._explore(actions)
            return ActionResult(
                action=action,
                decision=ActionType.EXPLORE,
                confidence=0.3,
                is_automatic=False,
                reasoning=f"íƒìƒ‰: '{action.name}' (ìƒˆë¡œìš´ ì‹œë„)"
            )
        
        # í™œìš©: Q-ê°’ ê¸°ë°˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì„ íƒ
        self.stats['deliberate_executions'] += 1
        action, confidence = self._exploit(actions)
        
        # Go/NoGo ê²°ì •
        decision = ActionType.GO if confidence > 0.3 else ActionType.NOGO
        
        return ActionResult(
            action=action,
            decision=decision,
            confidence=confidence,
            is_automatic=False,
            reasoning=f"ì„ íƒ: '{action.name}' (Q={action.q_value:.2f}, í™•ì‹ : {confidence:.2f})"
        )
    
    def _check_habit(self, context: str, possible_actions: List[str]) -> Optional[Action]:
        """ìŠµê´€ ì²´í¬"""
        if context not in self.q_table:
            return None
        
        for action_name in possible_actions:
            if action_name in self.q_table[context]:
                action = self.q_table[context][action_name]
                if action.is_habit:
                    return action
        
        return None
    
    def _get_or_create_actions(self, context: str, action_names: List[str]) -> List[Action]:
        """í–‰ë™ ê°ì²´ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        actions = []
        
        for name in action_names:
            if name in self.q_table[context]:
                actions.append(self.q_table[context][name])
            else:
                # ìƒˆ í–‰ë™ ìƒì„±
                action = Action(
                    name=name,
                    context=context,
                    q_value=self.params['exploration_bonus']  # ì´ˆê¸°ê°’ì— íƒìƒ‰ ë³´ë„ˆìŠ¤
                )
                self.q_table[context][name] = action
                actions.append(action)
        
        return actions
    
    def _should_explore(self) -> bool:
        """íƒìƒ‰í• ì§€ ê²°ì • (epsilon-greedy ìœ ì‚¬)"""
        # ë„íŒŒë¯¼ ë‚®ìœ¼ë©´ íƒìƒ‰ ì¦ê°€ (ìƒˆë¡œìš´ ë³´ìƒ ì°¾ê¸°)
        explore_prob = 0.1 + (1 - self.dopamine_level) * 0.2
        return random.random() < explore_prob
    
    def _explore(self, actions: List[Action]) -> Action:
        """íƒìƒ‰: ë‚®ì€ ì‹¤í–‰ íšŸìˆ˜ í–‰ë™ ì„ í˜¸"""
        # ì‹¤í–‰ íšŸìˆ˜ê°€ ì ì€ í–‰ë™ì— ê°€ì¤‘ì¹˜
        weights = [1.0 / (a.execution_count + 1) for a in actions]
        total = sum(weights)
        probs = [w / total for w in weights]
        
        return random.choices(actions, weights=probs)[0]
    
    def _exploit(self, actions: List[Action]) -> Tuple[Action, float]:
        """í™œìš©: Q-ê°’ ê¸°ë°˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì„ íƒ"""
        tau = self.params['tau']
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ í™•ë¥  ê³„ì‚°
        q_values = [a.q_value for a in actions]
        max_q = max(q_values) if q_values else 0
        
        # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ max ë¹¼ê¸°
        exp_values = [math.exp((q - max_q) / tau) for q in q_values]
        total = sum(exp_values)
        probs = [e / total for e in exp_values]
        
        # ì„ íƒ
        selected = random.choices(actions, weights=probs)[0]
        confidence = probs[actions.index(selected)]
        
        return selected, confidence
    
    def _normalize_context(self, context: str) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ ì •ê·œí™”
        
        v1.1: use_hash=True ì‹œ ê¸´ ë¬¸ìì—´ í•´ì‹± (ë©”ëª¨ë¦¬ ìµœì í™”)
        """
        normalized = context.lower().strip()
        
        if self.use_hash and len(normalized) > 50:
            # [v1.1] ê¸´ ì»¨í…ìŠ¤íŠ¸ëŠ” í•´ì‹œë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
            import hashlib
            return hashlib.md5(normalized.encode()).hexdigest()
        
        # ê¸°ë³¸: 50ìë¡œ ìë¦„ (ë””ë²„ê¹… ìš©ì´)
        return normalized[:50]
    
    # ============================================
    # 2. í•™ìŠµ (Learning)
    # ============================================
    
    def learn(self, context: str, action_name: str, reward: float, 
              next_context: str = None):
        """
        ë³´ìƒ í•™ìŠµ (TD-Learning)
        
        Q(s,a) â† Q(s,a) + Î±[R + Î³Â·max(Q(s',a')) - Q(s,a)]
        
        Args:
            context: ìƒí™©
            action_name: ì‹¤í–‰í•œ í–‰ë™
            reward: ë°›ì€ ë³´ìƒ (-1 ~ +1)
            next_context: ë‹¤ìŒ ìƒí™© (Noneì´ë©´ ì¢…ë£Œ ìƒíƒœ)
        """
        context = self._normalize_context(context)
        
        # í–‰ë™ ê°€ì ¸ì˜¤ê¸°
        if action_name not in self.q_table[context]:
            self.q_table[context][action_name] = Action(
                name=action_name, context=context
            )
        
        action = self.q_table[context][action_name]
        
        # ì‹¤í–‰ ê¸°ë¡
        action.execution_count += 1
        action.last_executed = time.time()
        
        if reward > 0:
            action.success_count += 1
        
        # TD ì—…ë°ì´íŠ¸
        alpha = self.params['alpha']
        gamma = self.params['gamma']
        
        # ë‹¤ìŒ ìƒíƒœì˜ ìµœëŒ€ Q-ê°’
        if next_context:
            next_context = self._normalize_context(next_context)
            next_q_values = [a.q_value for a in self.q_table[next_context].values()]
            max_next_q = max(next_q_values) if next_q_values else 0
        else:
            max_next_q = 0
        
        # Q-value ì—…ë°ì´íŠ¸
        td_error = reward + gamma * max_next_q - action.q_value
        action.q_value += alpha * td_error
        
        # ë„íŒŒë¯¼ ì—…ë°ì´íŠ¸ (TD error ê¸°ë°˜)
        self._update_dopamine(td_error)
        
        # ìŠµê´€ ê°•í™” (ì„±ê³µ ì‹œ)
        if reward > 0:
            self._strengthen_habit(action)
        elif reward < 0:
            self._weaken_habit(action)
        
        # ê¸°ë¡
        self.recent_actions.append((context, action_name, reward))
        self.recent_actions = self.recent_actions[-self.max_history:]
        self.stats['total_reward'] += reward
    
    def _update_dopamine(self, td_error: float):
        """ë„íŒŒë¯¼ ì—…ë°ì´íŠ¸ (TD error ê¸°ë°˜)"""
        # TD error > 0: ì˜ˆìƒë³´ë‹¤ ì¢‹ìŒ â†’ ë„íŒŒë¯¼ ì¦ê°€
        # TD error < 0: ì˜ˆìƒë³´ë‹¤ ë‚˜ì¨ â†’ ë„íŒŒë¯¼ ê°ì†Œ
        delta = td_error * 0.1
        self.dopamine_level = max(0, min(1, self.dopamine_level + delta))
        
        # ê¸°ì¤€ì„ ìœ¼ë¡œ ì„œì„œíˆ ë³µê·€
        decay = 0.05
        self.dopamine_level += decay * (self.dopamine_baseline - self.dopamine_level)
    
    def _strengthen_habit(self, action: Action):
        """ìŠµê´€ ê°•í™”"""
        beta = self.params['habit_beta']
        # H = H + Î²Â·(1 - H) â†’ ì ì§„ì ìœ¼ë¡œ 1ì— ì ‘ê·¼
        action.habit_strength += beta * (1 - action.habit_strength)
    
    def _weaken_habit(self, action: Action):
        """ìŠµê´€ ì•½í™”"""
        beta = self.params['habit_beta'] * 0.5  # ì•½í™”ëŠ” ë” ëŠë¦¬ê²Œ
        action.habit_strength = max(0, action.habit_strength - beta)
    
    # ============================================
    # 3. ìŠµê´€ ê´€ë¦¬
    # ============================================
    
    def get_habits(self) -> List[Action]:
        """ëª¨ë“  ìŠµê´€í™”ëœ í–‰ë™ ë°˜í™˜"""
        habits = []
        for context, actions in self.q_table.items():
            for action in actions.values():
                if action.is_habit:
                    habits.append(action)
        return habits
    
    def break_habit(self, context: str, action_name: str):
        """ìŠµê´€ ê¹¨ê¸°"""
        context = self._normalize_context(context)
        if context in self.q_table and action_name in self.q_table[context]:
            self.q_table[context][action_name].habit_strength = 0.0
    
    def decay_all(self):
        """ëª¨ë“  Q-ê°’ ê°ì‡  (ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í–‰ë™ ìŠê¸°)"""
        decay = self.params['decay_rate']
        for context, actions in self.q_table.items():
            for action in actions.values():
                action.q_value *= (1 - decay)
                # ë„ˆë¬´ ì˜¤ë˜ëœ ìŠµê´€ë„ ì•½í™”
                time_since = time.time() - action.last_executed
                if time_since > 3600:  # 1ì‹œê°„ ì´ìƒ
                    action.habit_strength *= 0.99
    
    # ============================================
    # 4. ìƒíƒœ ì¡°íšŒ
    # ============================================
    
    def get_best_action(self, context: str) -> Optional[Action]:
        """íŠ¹ì • ìƒí™©ì—ì„œ ìµœì„ ì˜ í–‰ë™"""
        context = self._normalize_context(context)
        if context not in self.q_table:
            return None
        
        actions = list(self.q_table[context].values())
        if not actions:
            return None
        
        return max(actions, key=lambda a: a.q_value)
    
    def get_state(self) -> Dict[str, Any]:
        """ì „ì²´ ìƒíƒœ ë°˜í™˜"""
        habits = self.get_habits()
        
        return {
            'dopamine': round(self.dopamine_level, 2),
            'total_contexts': len(self.q_table),
            'total_actions': sum(len(a) for a in self.q_table.values()),
            'habits': [
                {'context': h.context, 'action': h.name, 'strength': round(h.habit_strength, 2)}
                for h in habits[:5]  # ìƒìœ„ 5ê°œ
            ],
            'stats': self.stats,
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜"""
        return self.stats.copy()


# ============================================
# í…ŒìŠ¤íŠ¸
# ============================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§  Basal Ganglia (ê¸°ì €í•µ) í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    bg = BasalGanglia()
    
    # 1. í–‰ë™ ì„ íƒ í…ŒìŠ¤íŠ¸
    print("\nğŸ¯ [1] í–‰ë™ ì„ íƒ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    context = "ì¸ì‚¬ ìƒí™©"
    actions = ["ì•ˆë…•í•˜ì„¸ìš”", "ë°˜ê°‘ìŠµë‹ˆë‹¤", "í•˜ì´"]
    
    result = bg.select_action(context, actions)
    print(f"  ìƒí™©: '{context}'")
    print(f"  ê°€ëŠ¥í•œ í–‰ë™: {actions}")
    print(f"  ì„ íƒ: {result.action.name}")
    print(f"  ê²°ì •: {result.decision.value}")
    print(f"  ì´ìœ : {result.reasoning}")
    
    # 2. í•™ìŠµ í…ŒìŠ¤íŠ¸
    print("\nğŸ“š [2] í•™ìŠµ í…ŒìŠ¤íŠ¸ (ë³´ìƒ ê¸°ë°˜)")
    print("-" * 40)
    
    # ë°˜ë³µ í•™ìŠµ
    for i in range(20):
        # "ì•ˆë…•í•˜ì„¸ìš”"ì— ë†’ì€ ë³´ìƒ
        bg.learn(context, "ì•ˆë…•í•˜ì„¸ìš”", reward=0.8)
        # ë‹¤ë¥¸ í–‰ë™ì— ë‚®ì€ ë³´ìƒ
        bg.learn(context, "í•˜ì´", reward=0.2)
    
    print(f"  20íšŒ í•™ìŠµ í›„:")
    for action_name in actions:
        if action_name in bg.q_table[bg._normalize_context(context)]:
            action = bg.q_table[bg._normalize_context(context)][action_name]
            print(f"    '{action_name}': Q={action.q_value:.2f}, ìŠµê´€ê°•ë„={action.habit_strength:.2f}")
    
    # 3. ìŠµê´€ í˜•ì„± í…ŒìŠ¤íŠ¸
    print("\nâš¡ [3] ìŠµê´€ í˜•ì„± í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    # ë” ë§ì€ ë°˜ë³µ
    for i in range(30):
        bg.learn(context, "ì•ˆë…•í•˜ì„¸ìš”", reward=0.9)
    
    habits = bg.get_habits()
    print(f"  í˜•ì„±ëœ ìŠµê´€: {len(habits)}ê°œ")
    for h in habits:
        print(f"    '{h.context}' â†’ '{h.name}' (ê°•ë„: {h.habit_strength:.2f})")
    
    # 4. ìŠµê´€í™” í›„ í–‰ë™ ì„ íƒ
    print("\nğŸ”„ [4] ìŠµê´€í™” í›„ í–‰ë™ ì„ íƒ")
    print("-" * 40)
    
    result = bg.select_action(context, actions)
    print(f"  ì„ íƒ: {result.action.name}")
    print(f"  ìë™ ì‹¤í–‰: {result.is_automatic}")
    print(f"  ì´ìœ : {result.reasoning}")
    
    # 5. ë„íŒŒë¯¼ ìƒíƒœ
    print("\nğŸ’Š [5] ë„íŒŒë¯¼ ìƒíƒœ")
    print("-" * 40)
    print(f"  í˜„ì¬ ë„íŒŒë¯¼: {bg.dopamine_level:.2f}")
    
    # 6. ì „ì²´ ìƒíƒœ
    print("\nğŸ“Š [6] ì „ì²´ ìƒíƒœ")
    print("-" * 40)
    state = bg.get_state()
    print(f"  ì´ ì»¨í…ìŠ¤íŠ¸: {state['total_contexts']}")
    print(f"  ì´ í–‰ë™: {state['total_actions']}")
    print(f"  í†µê³„: {state['stats']}")
    
    print("\n" + "=" * 60)
    print("âœ… ê¸°ì €í•µ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)

