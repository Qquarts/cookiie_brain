"""
=============================================================================
Self-Organization Module: ìê¸°ì¡°ì§í™” ì‹œìŠ¤í…œ
=============================================================================

ğŸŒŠ ì² í•™:
    "í•˜ë“œì½”ë”©ì€ ì£½ìŒì´ë‹¤"
    "íŒ¨í„´ì€ ì§€ì •í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë°œê²¬í•˜ëŠ” ê²ƒì´ë‹¤"
    "í´ëŸ¬ìŠ¤í„°ëŠ” ìŠ¤ìŠ¤ë¡œ í˜•ì„±ëœë‹¤"

ğŸ“ í•µì‹¬ ì›ë¦¬:
    1. ê²½ìŸì  í•™ìŠµ (Competitive Learning)
       - ì…ë ¥ì— ê°€ì¥ ì˜ ë°˜ì‘í•˜ëŠ” ë‰´ëŸ°ì´ ìŠ¹ë¦¬
       - ìŠ¹ë¦¬í•œ ë‰´ëŸ°ì´ ì…ë ¥ íŒ¨í„´ì„ ë” ì˜ í‘œí˜„í•˜ë„ë¡ í•™ìŠµ
       
    2. ì¸¡ë©´ ì–µì œ (Lateral Inhibition)
       - ìŠ¹ë¦¬í•œ ë‰´ëŸ° ì£¼ë³€ì˜ ë‰´ëŸ°ë“¤ì€ ì–µì œ
       - í¬ì†Œ í‘œí˜„ (Sparse Coding) í˜•ì„±
       
    3. í—¤ë¹„ì•ˆ í•™ìŠµ (Hebbian Learning)
       - "í•¨ê»˜ ë°œí™”í•˜ëŠ” ë‰´ëŸ°ì€ í•¨ê»˜ ì—°ê²°ëœë‹¤"
       - ìì—°ìŠ¤ëŸ½ê²Œ íŒ¨í„´ í´ëŸ¬ìŠ¤í„° í˜•ì„±
       
    4. ë…¸ì´ì¦ˆ ê¸°ë°˜ íƒìƒ‰ (Noise-Driven Exploration)
       - ë…¸ì´ì¦ˆê°€ ìƒˆë¡œìš´ íŒ¨í„´ ë°œê²¬ì„ ì´‰ì§„
       - ê³ ì°©(local minimum) íƒˆì¶œ

ìƒë¬¼í•™ì  ê·¼ê±°:
    - ì‹œê° í”¼ì§ˆì˜ ë°©í–¥ ì„ íƒì„± (Orientation Selectivity)
    - í•´ë§ˆì˜ ì¥ì†Œ ì„¸í¬ (Place Cells)
    - ì†Œë‡Œì˜ ìš´ë™ íŒ¨í„´ í•™ìŠµ

ë¬¼ë¦¬í•™ì  ê·¼ê±°:
    - ì—´ì—­í•™ì  í‰í˜•ìœ¼ë¡œì˜ ìˆ˜ë ´
    - ì—ë„ˆì§€ ìµœì†Œí™” ì›ë¦¬
    - ìë°œì  ëŒ€ì¹­ ê¹¨ì§

Author: GNJz (Qquarts)
Version: 1.0.0
=============================================================================
"""

import numpy as np
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from collections import defaultdict
import time


@dataclass
class Pattern:
    """
    í•™ìŠµëœ íŒ¨í„´ (ìê¸°ì¡°ì§í™”ë¡œ ë°œê²¬ë¨)
    
    Attributes:
        id: íŒ¨í„´ ê³ ìœ  ID
        vector: íŒ¨í„´ ë²¡í„° (í”„ë¡œí† íƒ€ì…)
        activation_count: í™œì„±í™” íšŸìˆ˜
        last_activation: ë§ˆì§€ë§‰ í™œì„±í™” ì‹œê°„
        associated_labels: ì—°ê´€ëœ ë ˆì´ë¸”ë“¤
        strength: íŒ¨í„´ ê°•ë„ (ë°˜ë³µìœ¼ë¡œ ì¦ê°€)
    """
    id: str
    vector: np.ndarray
    activation_count: int = 0
    last_activation: float = 0.0
    associated_labels: List[str] = field(default_factory=list)
    strength: float = 1.0
    
    def __post_init__(self):
        if isinstance(self.vector, list):
            self.vector = np.array(self.vector)


class CompetitiveLearning:
    """
    ê²½ìŸì  í•™ìŠµ ë„¤íŠ¸ì›Œí¬
    
    ğŸ“ ì›ë¦¬:
        1. ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´ ëª¨ë“  ë‰´ëŸ°ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
        2. ê°€ì¥ ìœ ì‚¬í•œ ë‰´ëŸ°(ìŠ¹ì)ì´ ì…ë ¥ì„ í•™ìŠµ
        3. ì£¼ë³€ ë‰´ëŸ°ì€ ì–µì œ (WTA)
        
    ğŸ“ ìˆ˜ì‹:
        ìœ ì‚¬ë„: sim(x, w) = x Â· w / (|x| |w|)
        í•™ìŠµ: w â† w + Î·(x - w)  (ìŠ¹ìë§Œ)
    
    ìƒë¬¼í•™ì  ì˜ë¯¸:
        - íŠ¹ì • ìê·¹ì— ì„ íƒì ìœ¼ë¡œ ë°˜ì‘í•˜ëŠ” ë‰´ëŸ° í˜•ì„±
        - ì‹œê° í”¼ì§ˆì˜ ë°©í–¥ ì„ íƒì„±ê³¼ ìœ ì‚¬
    """
    
    def __init__(self, 
                 n_neurons: int = 100,
                 input_dim: int = 128,
                 learning_rate: float = 0.1):
        """
        Args:
            n_neurons: ë‰´ëŸ° ìˆ˜
            input_dim: ì…ë ¥ ì°¨ì›
            learning_rate: í•™ìŠµë¥ 
        """
        self.n_neurons = n_neurons
        self.input_dim = input_dim
        self.lr = learning_rate
        
        # ê°€ì¤‘ì¹˜ í–‰ë ¬ (ë¬´ì‘ìœ„ ì´ˆê¸°í™” - ë…¸ì´ì¦ˆê°€ ë‹¤ì–‘ì„±ì˜ ì”¨ì•—)
        self.weights = np.random.randn(n_neurons, input_dim)
        self._normalize_weights()
        
        # í™œì„±í™” ê¸°ë¡
        self.activations = np.zeros(n_neurons)
        
        # í•™ìŠµ í†µê³„
        self.stats = {
            'total_inputs': 0,
            'winner_history': [],
        }
        
    def _normalize_weights(self):
        """ê°€ì¤‘ì¹˜ ì •ê·œí™”"""
        norms = np.linalg.norm(self.weights, axis=1, keepdims=True)
        self.weights = self.weights / (norms + 1e-8)
    
    def forward(self, x: np.ndarray) -> Tuple[int, float]:
        """
        ìˆœì „íŒŒ - ìŠ¹ì ì„ íƒ
        
        ğŸ“ ìˆ˜ì‹:
            sim_i = x Â· w_i / (|x| |w_i|)
            winner = argmax(sim)
        
        Args:
            x: ì…ë ¥ ë²¡í„°
            
        Returns:
            (winner_idx, similarity): ìŠ¹ì ì¸ë±ìŠ¤ì™€ ìœ ì‚¬ë„
        """
        x = np.array(x).flatten()
        if len(x) != self.input_dim:
            raise ValueError(f"ì…ë ¥ ì°¨ì› ë¶ˆì¼ì¹˜: {len(x)} != {self.input_dim}")
        
        # ì •ê·œí™”
        x_norm = x / (np.linalg.norm(x) + 1e-8)
        
        # ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        similarities = self.weights @ x_norm
        
        # ìŠ¹ì ì„ íƒ
        winner = np.argmax(similarities)
        similarity = similarities[winner]
        
        # í™œì„±í™” ê¸°ë¡
        self.activations[winner] += 1
        self.stats['total_inputs'] += 1
        self.stats['winner_history'].append(winner)
        
        return int(winner), float(similarity)
    
    def learn(self, x: np.ndarray, winner: Optional[int] = None) -> int:
        """
        í•™ìŠµ - ìŠ¹ìê°€ ì…ë ¥ì„ í–¥í•´ ì´ë™
        
        ğŸ“ ìˆ˜ì‹:
            w_winner â† w_winner + Î·(x - w_winner)
        
        Args:
            x: ì…ë ¥ ë²¡í„°
            winner: ìŠ¹ì ì¸ë±ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ)
            
        Returns:
            winner_idx: ìŠ¹ì ì¸ë±ìŠ¤
        """
        x = np.array(x).flatten()
        x_norm = x / (np.linalg.norm(x) + 1e-8)
        
        # ìŠ¹ì ì„ íƒ
        if winner is None:
            winner, _ = self.forward(x)
        
        # í•™ìŠµ: w â† w + Î·(x - w)
        self.weights[winner] += self.lr * (x_norm - self.weights[winner])
        
        # ì •ê·œí™”
        self.weights[winner] /= np.linalg.norm(self.weights[winner]) + 1e-8
        
        return winner
    
    def get_most_activated(self, top_k: int = 5) -> List[Tuple[int, int]]:
        """ê°€ì¥ ë§ì´ í™œì„±í™”ëœ ë‰´ëŸ°ë“¤"""
        indices = np.argsort(self.activations)[::-1][:top_k]
        return [(int(i), int(self.activations[i])) for i in indices]


class HebbianCluster:
    """
    í—¤ë¹„ì•ˆ í´ëŸ¬ìŠ¤í„°ë§
    
    ğŸ“ ì›ë¦¬:
        "í•¨ê»˜ ë°œí™”í•˜ëŠ” ë‰´ëŸ°ì€ í•¨ê»˜ ì—°ê²°ëœë‹¤"
        (Neurons that fire together wire together)
        
    ğŸ“ ìˆ˜ì‹:
        Î”w_ij = Î· Â· x_i Â· x_j  (ë™ì‹œ í™œì„±í™”)
        
    ìƒë¬¼í•™ì  ì˜ë¯¸:
        - ìì—°ìŠ¤ëŸ¬ìš´ íŒ¨í„´ ê·¸ë£¹í™”
        - ì—°ê´€ ê¸°ì–µ í˜•ì„±
    """
    
    def __init__(self, n_neurons: int = 100, 
                 learning_rate: float = 0.01,
                 decay_rate: float = 0.001):
        """
        Args:
            n_neurons: ë‰´ëŸ° ìˆ˜
            learning_rate: í•™ìŠµë¥ 
            decay_rate: ê°ì‡ ìœ¨
        """
        self.n_neurons = n_neurons
        self.lr = learning_rate
        self.decay = decay_rate
        
        # ì—°ê²° ê°€ì¤‘ì¹˜ í–‰ë ¬ (ëŒ€ì¹­)
        self.connections = np.zeros((n_neurons, n_neurons))
        
        # í™œì„±í™” ê¸°ë¡
        self.active_history = []
        
    def activate(self, neurons: List[int]):
        """
        ë‰´ëŸ° ì§‘í•© í™œì„±í™” (ë™ì‹œ ë°œí™”)
        
        ğŸ“ ìˆ˜ì‹:
            ëª¨ë“  (i, j) ìŒì— ëŒ€í•´:
            w_ij â† w_ij + Î·  (i, j âˆˆ active_neurons)
        """
        self.active_history.append(set(neurons))
        
        # í—¤ë¹„ì•ˆ í•™ìŠµ: ë™ì‹œ í™œì„±í™”ëœ ë‰´ëŸ° ê°„ ì—°ê²° ê°•í™”
        for i in neurons:
            for j in neurons:
                if i != j:
                    self.connections[i, j] += self.lr
                    
    def decay_connections(self):
        """ì—°ê²° ê°ì‡ """
        self.connections *= (1 - self.decay)
        
    def get_clusters(self, threshold: float = 0.5) -> List[List[int]]:
        """
        í´ëŸ¬ìŠ¤í„° ì¶”ì¶œ (ê°•í•˜ê²Œ ì—°ê²°ëœ ê·¸ë£¹)
        
        Returns:
            í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸ (ê° í´ëŸ¬ìŠ¤í„°ëŠ” ë‰´ëŸ° ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸)
        """
        # ì—°ê²°ì´ threshold ì´ìƒì¸ ê²ƒë§Œ ê³ ë ¤
        strong = self.connections > threshold
        
        # Union-Findë¡œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
        visited = set()
        clusters = []
        
        for i in range(self.n_neurons):
            if i in visited:
                continue
            
            # BFSë¡œ ì—°ê²°ëœ ë‰´ëŸ° ì°¾ê¸°
            cluster = []
            queue = [i]
            while queue:
                node = queue.pop(0)
                if node in visited:
                    continue
                visited.add(node)
                cluster.append(node)
                
                # ê°•í•˜ê²Œ ì—°ê²°ëœ ì´ì›ƒ ì¶”ê°€
                for j in range(self.n_neurons):
                    if strong[node, j] and j not in visited:
                        queue.append(j)
            
            if len(cluster) > 1:  # ë‹¨ì¼ ë‰´ëŸ°ì€ í´ëŸ¬ìŠ¤í„° ì•„ë‹˜
                clusters.append(sorted(cluster))
        
        return clusters


class PatternMemory:
    """
    ìê¸°ì¡°ì§í™” íŒ¨í„´ ë©”ëª¨ë¦¬
    
    ğŸ“ ì›ë¦¬:
        1. ìƒˆë¡œìš´ íŒ¨í„´ì´ ë“¤ì–´ì˜¤ë©´ ê¸°ì¡´ íŒ¨í„´ê³¼ ë¹„êµ
        2. ìœ ì‚¬í•œ íŒ¨í„´ì´ ìˆìœ¼ë©´ ë³‘í•© (ì¼ë°˜í™”)
        3. ìƒˆë¡œìš´ íŒ¨í„´ì´ë©´ ì €ì¥ (ë¶„í™”)
        4. ì˜¤ë˜ ì‚¬ìš© ì•ˆ ëœ íŒ¨í„´ì€ ì•½í™” (ë§ê°)
        
    ğŸŒŠ ì² í•™:
        - í•˜ë“œì½”ë”©ëœ íŒ¨í„´ ëŒ€ì‹  í•™ìŠµëœ íŒ¨í„´ ì‚¬ìš©
        - íŒ¨í„´ì€ ë°œê²¬ë˜ëŠ” ê²ƒì´ì§€ ì§€ì •ë˜ëŠ” ê²ƒì´ ì•„ë‹˜
    """
    
    def __init__(self, 
                 pattern_dim: int = 128,
                 similarity_threshold: float = 0.8,
                 max_patterns: int = 1000):
        """
        Args:
            pattern_dim: íŒ¨í„´ ì°¨ì›
            similarity_threshold: ë³‘í•© ì„ê³„ê°’
            max_patterns: ìµœëŒ€ íŒ¨í„´ ìˆ˜
        """
        self.pattern_dim = pattern_dim
        self.threshold = similarity_threshold
        self.max_patterns = max_patterns
        
        self.patterns: Dict[str, Pattern] = {}
        self.pattern_count = 0
        
    def _generate_id(self) -> str:
        """ê³ ìœ  ID ìƒì„±"""
        self.pattern_count += 1
        return f"P{self.pattern_count:04d}"
    
    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
        a = np.array(a).flatten()
        b = np.array(b).flatten()
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        if norm_a < 1e-8 or norm_b < 1e-8:
            return 0.0
        return float(np.dot(a, b) / (norm_a * norm_b))
    
    def find_similar(self, vector: np.ndarray) -> Optional[Pattern]:
        """
        ê°€ì¥ ìœ ì‚¬í•œ íŒ¨í„´ ì°¾ê¸°
        
        Args:
            vector: ì…ë ¥ ë²¡í„°
            
        Returns:
            ê°€ì¥ ìœ ì‚¬í•œ íŒ¨í„´ (ì„ê³„ê°’ ì´ìƒì¼ ë•Œë§Œ)
        """
        best_pattern = None
        best_similarity = 0.0
        
        for pattern in self.patterns.values():
            sim = self._cosine_similarity(vector, pattern.vector)
            if sim > best_similarity and sim >= self.threshold:
                best_similarity = sim
                best_pattern = pattern
        
        return best_pattern
    
    def learn(self, vector: np.ndarray, label: Optional[str] = None) -> Pattern:
        """
        íŒ¨í„´ í•™ìŠµ (ìê¸°ì¡°ì§í™”)
        
        ğŸ“ ê·œì¹™:
            1. ìœ ì‚¬í•œ íŒ¨í„´ ìˆìŒ â†’ ë³‘í•© (í”„ë¡œí† íƒ€ì… ì—…ë°ì´íŠ¸)
            2. ìœ ì‚¬í•œ íŒ¨í„´ ì—†ìŒ â†’ ìƒˆ íŒ¨í„´ ìƒì„±
        
        Args:
            vector: ì…ë ¥ ë²¡í„°
            label: ë ˆì´ë¸” (ì„ íƒ)
            
        Returns:
            í•™ìŠµëœ/ë³‘í•©ëœ íŒ¨í„´
        """
        vector = np.array(vector).flatten()
        now = time.time()
        
        # ìœ ì‚¬í•œ íŒ¨í„´ ì°¾ê¸°
        similar = self.find_similar(vector)
        
        if similar:
            # ë³‘í•©: í”„ë¡œí† íƒ€ì… ì—…ë°ì´íŠ¸ (ì´ë™ í‰ê· )
            alpha = 0.1  # í•™ìŠµë¥ 
            similar.vector = (1 - alpha) * similar.vector + alpha * vector
            similar.activation_count += 1
            similar.last_activation = now
            similar.strength = min(10.0, similar.strength + 0.1)
            
            if label and label not in similar.associated_labels:
                similar.associated_labels.append(label)
            
            return similar
        else:
            # ìƒˆ íŒ¨í„´ ìƒì„±
            pattern_id = self._generate_id()
            pattern = Pattern(
                id=pattern_id,
                vector=vector.copy(),
                activation_count=1,
                last_activation=now,
                associated_labels=[label] if label else [],
                strength=1.0
            )
            self.patterns[pattern_id] = pattern
            
            # ìµœëŒ€ íŒ¨í„´ ìˆ˜ ì´ˆê³¼ ì‹œ ê°€ì¥ ì•½í•œ íŒ¨í„´ ì œê±°
            if len(self.patterns) > self.max_patterns:
                self._prune_weakest()
            
            return pattern
    
    def _prune_weakest(self):
        """ê°€ì¥ ì•½í•œ íŒ¨í„´ ì œê±°"""
        if not self.patterns:
            return
        
        weakest_id = min(self.patterns.keys(), 
                        key=lambda k: self.patterns[k].strength)
        del self.patterns[weakest_id]
    
    def decay(self, rate: float = 0.01):
        """íŒ¨í„´ ê°•ë„ ê°ì‡ """
        for pattern in self.patterns.values():
            pattern.strength = max(0.1, pattern.strength - rate)
    
    def get_strongest(self, top_k: int = 10) -> List[Pattern]:
        """ê°€ì¥ ê°•í•œ íŒ¨í„´ë“¤"""
        sorted_patterns = sorted(self.patterns.values(), 
                                 key=lambda p: p.strength, 
                                 reverse=True)
        return sorted_patterns[:top_k]
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„"""
        return {
            'total_patterns': len(self.patterns),
            'avg_strength': np.mean([p.strength for p in self.patterns.values()]) if self.patterns else 0,
            'avg_activations': np.mean([p.activation_count for p in self.patterns.values()]) if self.patterns else 0,
        }


# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================

def text_to_vector(text: str, dim: int = 128) -> np.ndarray:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜)
    
    Note: ì‹¤ì œë¡œëŠ” Word2Vec, BERT ë“± ì‚¬ìš© ê¶Œì¥
    """
    import hashlib
    
    if not text:
        return np.zeros(dim)
    
    # í•´ì‹œ ê¸°ë°˜ ì‹œë“œ ìƒì„±
    seed = int(hashlib.md5(text.encode()).hexdigest()[:8], 16)
    np.random.seed(seed)
    
    # ëœë¤ ë²¡í„° ìƒì„± ë° ì •ê·œí™”
    vec = np.random.randn(dim)
    return vec / (np.linalg.norm(vec) + 1e-8)


# =============================================================================
# í…ŒìŠ¤íŠ¸
# =============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸŒŠ Self-Organization Test")
    print("=" * 60)
    
    # 1. ê²½ìŸì  í•™ìŠµ
    print("\n1ï¸âƒ£ Competitive Learning...")
    cl = CompetitiveLearning(n_neurons=10, input_dim=8)
    
    # ëª‡ ê°€ì§€ íŒ¨í„´ í•™ìŠµ
    patterns = [
        np.array([1, 1, 0, 0, 0, 0, 0, 0]),
        np.array([0, 0, 1, 1, 0, 0, 0, 0]),
        np.array([0, 0, 0, 0, 1, 1, 0, 0]),
    ]
    
    for _ in range(50):
        for p in patterns:
            cl.learn(p + np.random.randn(8) * 0.1)  # ë…¸ì´ì¦ˆ ì¶”ê°€
    
    print(f"   ê°€ì¥ í™œì„±í™”ëœ ë‰´ëŸ°: {cl.get_most_activated(3)}")
    
    # 2. í—¤ë¹„ì•ˆ í´ëŸ¬ìŠ¤í„°
    print("\n2ï¸âƒ£ Hebbian Clustering...")
    hc = HebbianCluster(n_neurons=10)
    
    # ë™ì‹œ í™œì„±í™” íŒ¨í„´
    for _ in range(20):
        hc.activate([0, 1, 2])  # í´ëŸ¬ìŠ¤í„° 1
        hc.activate([5, 6, 7])  # í´ëŸ¬ìŠ¤í„° 2
    
    clusters = hc.get_clusters(threshold=0.1)
    print(f"   ë°œê²¬ëœ í´ëŸ¬ìŠ¤í„°: {clusters}")
    
    # 3. íŒ¨í„´ ë©”ëª¨ë¦¬
    print("\n3ï¸âƒ£ Pattern Memory...")
    pm = PatternMemory(pattern_dim=8)
    
    # íŒ¨í„´ í•™ìŠµ
    for i in range(10):
        vec = patterns[i % 3] + np.random.randn(8) * 0.1
        pm.learn(vec, label=f"pattern_{i % 3}")
    
    print(f"   ì €ì¥ëœ íŒ¨í„´ ìˆ˜: {pm.get_stats()['total_patterns']}")
    print(f"   ê°€ì¥ ê°•í•œ íŒ¨í„´: {pm.get_strongest(2)}")
    
    print("\n" + "=" * 60)
    print("âœ… ìê¸°ì¡°ì§í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)

